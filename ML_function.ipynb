{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMgpYVYoisFz"
   },
   "source": [
    "# Recon8D Example \n",
    "##### Author: Ryan Schildcrout\n",
    "\n",
    "## Summary\n",
    "##### This notebook generates the machine learning models used in Schildcrout, R., Smith, K., Bhowmick, R., Lu, Y., Menon, S., Kapadia, M., Kurtz, E., Coffeen-Vandeven, A., Nelakuditi, S., & Chandrasekaran, S. (2025). Recon8D: A metabolic regulome network from oct-omics and machine learning (p. 2024.08.17.608400). bioRxiv. https://doi.org/10.1101/2024.08.17.608400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMgpYVYoisFz"
   },
   "source": [
    "## 1. Load in necessary packages and data\n",
    "##### This notebook was written to work in a conda environment. All required packages to generate Recon8D models are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xL_j1L6cjOu6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import dill\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMgpYVYoisFz"
   },
   "source": [
    "##### Sample data (using histone PTMs to predict metabolome variation) can be found in the example_datasets folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkeOAmuUjwQc"
   },
   "outputs": [],
   "source": [
    "# Read in data (rows = cell lines, columns = features)\n",
    "input_features = pd.read_csv('./your_file_here.csv',index_col=0)\n",
    "metabolomics = pd.read_csv('./your_metabolomics_here.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMgpYVYoisFz"
   },
   "source": [
    "## 2. Data preprocessing\n",
    "##### A. Remove features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkeOAmuUjwQc"
   },
   "outputs": [],
   "source": [
    "# Function for removing features with low variance\n",
    "def remove_lowVar(df, minVariance):\n",
    "  selector = VarianceThreshold(threshold = minVariance) \n",
    "  selector.fit(df)\n",
    "  df_new = df.loc[:, selector.get_support()]\n",
    "  print(\"Removed {} of {} metabolites for having zero variance\".format(df.shape[1]- df_new.shape[1], df.shape[1]),flush=True)\n",
    "  return(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkeOAmuUjwQc"
   },
   "outputs": [],
   "source": [
    "# Call function to remove features with zero variance\n",
    "print(\"Your_feature_type_here...\")\n",
    "feature_highVar = remove_lowVar(input_features, 0)\n",
    "\n",
    "print(\"Metabolites...\")\n",
    "met_highVar = remove_lowVar(metabolomics, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMgpYVYoisFz"
   },
   "source": [
    "##### B. Make lists of feature names and metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkeOAmuUjwQc"
   },
   "outputs": [],
   "source": [
    "feature_names = feature_highVar.columns\n",
    "metabolite_names = met_highVar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMgpYVYoisFz"
   },
   "source": [
    "##### C. Merge datasets by cell line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkeOAmuUjwQc"
   },
   "outputs": [],
   "source": [
    "print(\"\\nFEATURE:\")\n",
    "print(feature_highVar.shape)\n",
    "print(met_highVar.shape)\n",
    "merged_data =  pd.merge(feature_highVar, met_highVar, left_index=True, right_index=True)\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ8Z5RgsUrCA"
   },
   "source": [
    "## 3. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ8Z5RgsUrCA"
   },
   "source": [
    "##### A. Function for RandomForests and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuAaod98iFJL"
   },
   "outputs": [],
   "source": [
    "# Set scaler (for z-scoring)\n",
    "scaler = StandardScaler()\n",
    "def train_multiRegressCV(classifier, data, x_cols, y_cols, n_splits=5, pval_cutoff=0.05, scale_y=True):\n",
    "\n",
    "  # Identify features and metabolites from input lists\n",
    "  X =  data.loc[:, x_cols]\n",
    "  y =  data.loc[:, y_cols]\n",
    "\n",
    "  # Set the classifier for the type of algorithm entered\n",
    "  if classifier==\"RF\":\n",
    "    model=RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1, max_depth=10)\n",
    "    print(\"Training model with random forest!\")\n",
    "  elif classifier==\"XGBoost\":\n",
    "    model=XGBRegressor(random_state=0,n_estimators=100,n_jobs=-1)\n",
    "    print(\"Training model with XGBoost\")\n",
    "  else:\n",
    "    print(\"Enter model type!\")\n",
    "    sys.exit()\n",
    "\n",
    "  # Create multioutput model (one model trained for each metabolite)\n",
    "  clf = MultiOutputRegressor(model)\n",
    "\n",
    "  # Create cross validation splits\n",
    "  cv = KFold(n_splits=n_splits,\n",
    "                shuffle=True,\n",
    "                random_state=123)\n",
    "\n",
    "  # Create empty lists to store test and prediction data\n",
    "  df_y_test_all = pd.DataFrame(columns=y_cols)\n",
    "  df_y_pred_all = pd.DataFrame(columns=y_cols)\n",
    "\n",
    "  count = 1\n",
    "\n",
    "  # loop through cross-val folds\n",
    "  for train_index, test_index in cv.split(X, y):\n",
    "      \n",
    "      # Start timer\n",
    "      start_time = time.perf_counter()\n",
    "\n",
    "      print(\"CV fold:\")\n",
    "      print(count,flush=True)\n",
    "\n",
    "      # Set train and test data for CV splits\n",
    "      X_trainCV, X_testCV = X.iloc[train_index], X.iloc[test_index]\n",
    "      y_trainCV, y_testCV = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      # Scale data (z-score) based on training set\n",
    "      X_trainCV =  scaler.fit_transform(X_trainCV)\n",
    "      X_testCV =  scaler.transform(X_testCV)\n",
    "\n",
    "      if scale_y:\n",
    "        y_trainCV =  scaler.fit_transform(y_trainCV)\n",
    "        y_testCV =  scaler.transform(y_testCV)\n",
    "\n",
    "      # Train ML model!\n",
    "      tmp_mdl = clf.fit(X_trainCV, y_trainCV)\n",
    "      y_predCV = tmp_mdl.predict(X_testCV)\n",
    "\n",
    "      df_pred_temp  = pd.DataFrame(y_predCV, columns=y_cols)\n",
    "      df_test_temp = pd.DataFrame(y_testCV, columns=y_cols)\n",
    "\n",
    "      # Concatenate data from each CV\n",
    "      df_y_test_all = pd.concat([df_y_test_all, df_test_temp])\n",
    "      df_y_pred_all = pd.concat([df_y_pred_all, df_pred_temp])\n",
    "\n",
    "      count = count+1\n",
    "\n",
    "      finish_time = time.perf_counter()\n",
    "      print(\"CV fold finished in {} seconds\".format(finish_time-start_time),flush=True)\n",
    "\n",
    "\n",
    "  print(\"Calculating Pearson's R for each metabolite...\")\n",
    "\n",
    "  # Calculate Pearson's R for predicted vs test data\n",
    "  r = []\n",
    "  for i, col in enumerate(y_cols):\n",
    "    r.append(scipy.stats.pearsonr(df_y_test_all.iloc[:,i], df_y_pred_all.iloc[:,i]))\n",
    "\n",
    "  df_results = pd.DataFrame(r, columns=['R','pval'], index=y_cols)\n",
    "\n",
    "  # Determine significance based on FDR-corrected P value\n",
    "  # R must be positive, as we're looking at true vs predicted values\n",
    "  df_results['Significant']  = (df_results.pval < pval_cutoff) & (df_results.R > 0)\n",
    "  df_results['R2']  = df_results.R**2\n",
    "\n",
    "  # Scale all data\n",
    "  X_final = scaler.fit_transform(X)\n",
    "  X_final = pd.DataFrame(X_final,columns=X.columns,index=X.index)\n",
    "  if scale_y:\n",
    "    y_final = scaler.fit_transform(y)\n",
    "    y_final = pd.DataFrame(y_final,columns=y.columns,index=y.index)\n",
    "\n",
    "  # Train final model on all data\n",
    "  print(\"Training final model on dataset of {} samples and {} features\".format(X_final.shape[0], X_final.shape[1]))\n",
    "  final_mdl = clf.fit(X_final, y_final)\n",
    "\n",
    "  # Output: dataframe with results, multi-output model containing models for each metabolite\n",
    "  return df_results, final_mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ8Z5RgsUrCA"
   },
   "source": [
    "##### B. Function for Ridge and Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYgI-J2YUvig"
   },
   "outputs": [],
   "source": [
    "# Set scaler (for z-scoring)\n",
    "scaler = StandardScaler()\n",
    "def train_tune_CV(classifier,data, x_cols, y_cols,\n",
    "                  n_splits=5, pval_cutoff=0.05, alphas=[1e-2, 0.1, 1, 10, 100, 1000], scale_y=True):\n",
    "\n",
    "  # Identify features and metabolites from input lists\n",
    "  X =  data.loc[:, x_cols]\n",
    "  y =  data.loc[:, y_cols]\n",
    "\n",
    "  # Set the classifier for the type of algorithm entered\n",
    "  if classifier==\"Ridge\":\n",
    "    model=Ridge(alpha=1.0, max_iter=None, tol=0.001, solver='auto', random_state=0)\n",
    "    print(\"Training model with Ridge!\")\n",
    "  elif classifier==\"Lasso\":\n",
    "    model=Lasso(alpha=1.0, max_iter=500, tol=0.001, random_state=0)\n",
    "    print(\"Training model with Lasso!\")\n",
    "  else:\n",
    "    print(\"Enter model type!\")\n",
    "    sys.exit()\n",
    "\n",
    "  # Create multioutput model (one model trained for each metabolite)\n",
    "  multi_ridge = MultiOutputRegressor(model)\n",
    "\n",
    "  # Iterate through alphas\n",
    "  hyperParameters = {'estimator__alpha':alphas}\n",
    "  gridSearch = GridSearchCV(multi_ridge, hyperParameters, scoring='r2', cv=5)\n",
    "\n",
    "  cv = KFold(n_splits=n_splits,\n",
    "                shuffle=True,\n",
    "                random_state=123)\n",
    "\n",
    "  # Create empty lists to store test and prediction data\n",
    "  df_y_test_all = pd.DataFrame(columns=y_cols)\n",
    "  df_y_pred_all = pd.DataFrame(columns=y_cols)\n",
    "\n",
    "  # Initialize dataframes for hyperparameters\n",
    "  df_alpha_cv = pd.DataFrame()\n",
    "  df_r_cv = pd.DataFrame()\n",
    "\n",
    "  # Loop through cross-val folds\n",
    "  count = 1\n",
    "\n",
    "  for count, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
    "      print(count)\n",
    "      X_trainCV, X_testCV = X.iloc[train_index], X.iloc[test_index]\n",
    "      y_trainCV, y_testCV = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      # Scale data (z-score) based on training set\n",
    "      X_trainCV =  scaler.fit_transform(X_trainCV)\n",
    "      X_testCV =  scaler.transform(X_testCV)\n",
    "\n",
    "      if scale_y:\n",
    "        y_trainCV = scaler.fit_transform(y_trainCV)\n",
    "        y_testCV = scaler.transform(y_testCV)\n",
    "\n",
    "      # Train ML model!\n",
    "      tmp_mdl = gridSearch.fit(X_trainCV, y_trainCV)\n",
    "\n",
    "      # predict on CV-test set\n",
    "      y_predCV = tmp_mdl.predict(X_testCV)\n",
    "\n",
    "      # Store tuned alpha values\n",
    "      my_alphas = []\n",
    "      r_cv = []\n",
    "\n",
    "      for i in range(len(tmp_mdl.best_estimator_.estimators_)):\n",
    "        my_alphas.append(tmp_mdl.best_estimator_.estimators_[i].get_params()['alpha'])\n",
    "        r_cv.append(scipy.stats.pearsonr(y_predCV[:,i], y_testCV[:,i])[0])\n",
    "\n",
    "      df_alpha_cv.loc[:,count] = my_alphas\n",
    "      df_r_cv.loc[:,count] = r_cv\n",
    "\n",
    "      df_pred_temp  = pd.DataFrame(y_predCV, columns=y_cols)\n",
    "      df_test_temp = pd.DataFrame(y_testCV, columns=y_cols)\n",
    "\n",
    "      # Concatenate data from each CV\n",
    "      df_y_test_all = pd.concat([df_y_test_all, df_test_temp])\n",
    "      df_y_pred_all = pd.concat([df_y_pred_all, df_pred_temp])\n",
    "\n",
    "  print(\"Calculating Pearson's R for each metabolite...\")\n",
    "  \n",
    "  # Calculate Pearson's R for predicted vs test data\n",
    "  r = []\n",
    "  for i, col in enumerate(y_cols):\n",
    "    r.append(scipy.stats.pearsonr(df_y_test_all.iloc[:,i], df_y_pred_all.iloc[:,i]))\n",
    "\n",
    "  df_results = pd.DataFrame(r, columns=['R','pval'], index=y_cols)\n",
    "\n",
    "  # Determine significance based on FDR-corrected P value\n",
    "  # R must be positive, as we're looking at true vs predicted values\n",
    "  df_results['Significant']  = (df_results.pval < pval_cutoff) & (df_results.R > 0)\n",
    "  df_results['R2']  = df_results.R**2\n",
    "\n",
    "  # Select best hyperparameters\n",
    "  print(\"For each metabolite, find CV fold with best R and get alpha...\")\n",
    "  ix_best_fold = df_r_cv.idxmax(axis=1)\n",
    "  best_alphas = df_alpha_cv.values[np.arange(len(ix_best_fold)),ix_best_fold]\n",
    "  Counter(best_alphas)\n",
    "  savetxt('best_alphas_phos.csv', best_alphas)\n",
    "\n",
    "  print(\"Training final models with best alphas...\")\n",
    "  \n",
    "  # Scale all data\n",
    "  X_final = scaler.fit_transform(X)\n",
    "  X_final = pd.DataFrame(X_final,columns=X.columns,index=X.index)\n",
    "  if scale_y:\n",
    "    y_final = scaler.fit_transform(y)\n",
    "    y_final = pd.DataFrame(y_final,columns=y.columns,index=y.index)\n",
    "\n",
    "  # Train final model on all data with proper alpha values\n",
    "  final_mdls = list()\n",
    "  for i in range(len(best_alphas)):\n",
    "    if classifier==\"Ridge\":\n",
    "        model=Ridge(alpha=best_alphas[i], max_iter=None, tol=0.001, solver='auto', random_state=0)\n",
    "    else:\n",
    "        model=Lasso(alpha=best_alphas[i], max_iter=500, tol=0.001, random_state=0)\n",
    "    model.fit(X_final.iloc[:,:], y_final.iloc[:,i])\n",
    "    final_mdls.append(model)\n",
    "\n",
    "  # Output: dataframe with results, multi-output model containing models for each metabolite, and the best alpha values\n",
    "  return df_results, final_mdls, best_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ8Z5RgsUrCA"
   },
   "source": [
    "##### C. P value correction and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8ChGdKyk1pT"
   },
   "outputs": [],
   "source": [
    "# Assign significance value (Bonferroni correction) for multiple hypothesis testing\n",
    "pval_cutoff = 0.05 / len(metabolite_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ImyA4F_lFQb"
   },
   "outputs": [],
   "source": [
    "# Call ML function (either train_multiRegressCV for RF or XGB, or train_tune_CV for Ridge or Lasso)\n",
    "results_df, model = train_multiRegressCV(\"RF\",\n",
    "                                            data = merged_data, # Whole merged data set\n",
    "                                            x_cols = feature_names, # List of feature names\n",
    "                                            y_cols = metabolite_names, # List of variable names\n",
    "                                            pval_cutoff = pval_cutoff, # Significance value\n",
    "                                            n_splits = 5) # Cross validation splits\n",
    "\n",
    "print(\"Metabolites below Bonferroni pval cutoff: {}\".format(np.sum(results_df.Significant)),flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ImyA4F_lFQb"
   },
   "outputs": [],
   "source": [
    "# Save model results\n",
    "df = pd.DataFrame({\"pearsons_r\":results_df.R,\n",
    "                                \"model_pval\":results_df.pval,\n",
    "                                \"metabolite_significant\":results_df.Significant})\n",
    "df.to_csv(\"./your_results.csv\")\n",
    "\n",
    "# Save models\n",
    "print(\"Saving models!\",flush=True)\n",
    "with open('./your_final_model.pkl', 'wb') as f:\n",
    "   dill.dump(model, f)\n",
    "print(\"Done!\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4jt5pOOPhg5"
   },
   "source": [
    "## 4. Generate feature importances from RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in RF models\n",
    "print(\"Loading models!\")\n",
    "with open('./results/your_final_model.pkl', 'rb') as f:\n",
    "   final_models = dill.load(f)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Pull first set of feature importances\n",
    "importances = pd.DataFrame(final_models.estimators_[0].feature_importances_,index=feature_names)\n",
    "# Append feature importances for all other metabolites\n",
    "for i in range(0,224):\n",
    "  importances = final_models.estimators_[i+1].feature_importances_\n",
    "    # Convert to dataframe\n",
    "  importances_df = pd.DataFrame(importances)\n",
    "    # Label features\n",
    "  importances_df = importances_df.set_index(feature_names)\n",
    "    # Append features from next metabolite model\n",
    "  importances[str(i+1)] = importances_df\n",
    "  \n",
    "importances = importances.set_axis(metabolite_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4jt5pOOPhg5"
   },
   "source": [
    "## 5. Metabolome prediction using histone PTMs example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeCBuztsPtUd"
   },
   "outputs": [],
   "source": [
    "# Set file paths and read in your feature and variable data\n",
    "features_file_path = \"./CCLE_hist.csv\"\n",
    "metabolomics_file_path = \"./CCLE_metabolomics_averages.csv\"\n",
    "input_features = pd.read_csv(features_file_path, index_col=0)\n",
    "metabolomics = pd.read_csv(metabolomics_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_8qL4_VgLya"
   },
   "outputs": [],
   "source": [
    "# Call function to remove features with zero variance\n",
    "print(\"Histone PTMs...\")\n",
    "feature_highVar = remove_lowVar(input_features, 0)\n",
    "\n",
    "print(\"Metabolites...\")\n",
    "met_highVar = remove_lowVar(metabolomics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_8qL4_VgLya"
   },
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = input_features.columns\n",
    "my_metabs = metabolomics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ed8uOKIgR1P"
   },
   "outputs": [],
   "source": [
    "# Merge features+metabs by cell lines\n",
    "print(\"\\nFEATURE:\")\n",
    "print(feature_highVar.shape)\n",
    "print(met_highVar.shape)\n",
    "merged_data =  pd.merge(feature_highVar, met_highVar, left_index=True, right_index=True)\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5y8IysBhDZ3"
   },
   "outputs": [],
   "source": [
    "# Assign significance value (Bonferroni correction)\n",
    "pval_cutoff_ccle = 0.05 / len(my_metabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PmhGw98EhFX_"
   },
   "outputs": [],
   "source": [
    "# Train ML models and save results\n",
    "results_df, model = train_multiRegressCV(\"RF\",\n",
    "                                            data = merged_data,\n",
    "                                            x_cols = feature_names,\n",
    "                                            y_cols = my_metabs,\n",
    "                                            pval_cutoff = pval_cutoff_ccle,\n",
    "                                            n_splits = 5)\n",
    "\n",
    "print(\"Metabolites below Bonferroni pval cutoff: {}\".format(np.sum(results_df.Significant)),flush=True)\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame({\"pearsons_r\":results_df.R,\n",
    "                                \"model_pval\":results_df.pval,\n",
    "                                \"metabolite_significant\":results_df.Significant})\n",
    "df.to_csv(\"./CCLE_histone_results.csv\")\n",
    "\n",
    "# Save models\n",
    "print(\"Saving models!\",flush=True)\n",
    "with open('./CCLE_histone_model.pkl', 'wb') as f:\n",
    "   dill.dump(model, f)\n",
    "print(\"Done!\",flush=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
